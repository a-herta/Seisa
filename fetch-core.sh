#!/system/bin/sh
# =====================================================================
# ğŸ“¥ fetch-core.sh - æ™ºèƒ½ä¸‹è½½ä»£ç†æ ¸å¿ƒçš„è„šæœ¬
# =====================================================================

set -e

MODDIR=$(dirname "$0")
. "$MODDIR/common.sh"

# --- è¾“å…¥å‚æ•°æ ¡éªŒ ---
if [ "$#" -lt 3 ]; then
  echo "::error::ç”¨æ³•: $0 <repo> <bin_name> <arch> <output_path>" >&2
  exit 1
fi

BIN_REPO="$1"
BIN_NAME="$2"
ARCHITECTURE="$3"
OUTPUT_PATH="$4"

MAX_RETRIES=${MAX_RETRIES:-3}
RETRY_DELAY=${RETRY_DELAY:-5}
TMPDIR=$(mktemp -d "${PERSIST_DIR}/.tmp.XXXXXX" 2>/dev/null || mktemp -d)
trap 'rm -rf "$TMPDIR"' EXIT

# --- å‡½æ•° ---
log_safe() {
  echo "$@" >&2
}

# å¸¦é‡è¯•çš„ curl
retry_curl() {
  url="$1" output_path="$2" count=0
  while [ "$count" -lt "$MAX_RETRIES" ]; do
    if [ -n "$AUTH_HDR" ]; then
      curl -sSL -H "Accept: application/vnd.github.v3+json" -H "$AUTH_HDR" "$url" -o "$output_path" && [ -s "$output_path" ] && return 0
    else
      curl -sSL -H "Accept: application/vnd.github.v3+json" "$url" -o "$output_path" && [ -s "$output_path" ] && return 0
    fi
    count=$((count + 1))
    [ "$count" -ge "$MAX_RETRIES" ] && log_safe "âŒ ä¸‹è½½å¤±è´¥: $url" && return 1
    log_safe "â³ ä¸‹è½½å¤±è´¥, $RETRY_DELAY ç§’åé‡è¯• ($count/$MAX_RETRIES)..."
    sleep "$RETRY_DELAY"
  done
}

# é€’å½’è§£å‹, ç›´åˆ°æ‰¾åˆ°ç›®æ ‡äºŒè¿›åˆ¶æ–‡ä»¶æˆ–æ— æ³•å†è§£å‹
decompress_recursively() {
  file_path="$1"
  target_dir="$2"
  found_path=""

  while true; do
    # æ£€æŸ¥å½“å‰ç›®å½•ä¸‹æ˜¯å¦å·²å­˜åœ¨ç›®æ ‡æ–‡ä»¶
    found_path=$(find "$target_dir" -type f -iname "$BIN_NAME" | head -n 1)
    [ -n "$found_path" ] && echo "$found_path" && return 0

    # å¯»æ‰¾ä¸‹ä¸€ä¸ªéœ€è¦è§£å‹çš„æ–‡ä»¶
    if file "$file_path" | grep -qi 'gzip compressed data'; then
      compressed_file="$file_path"
      log_safe "ğŸ—œï¸ tar.gz å‹ç¼©åŒ…, è§£å‹ä¸­..."
      # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å­ç›®å½•æ¥è§£å‹, é¿å…æ–‡ä»¶åå†²çª
      sub_dir=$(mktemp -d "$target_dir/decompress.XXXXXX")
      tar -xzf "$compressed_file" -C "$sub_dir"
      # åˆ é™¤å·²è§£å‹çš„å‹ç¼©åŒ…, ä»¥ä¾¿ä¸‹æ¬¡å¾ªç¯å¤„ç†æ–°æ–‡ä»¶
      rm -f "$compressed_file"
      # æ›´æ–° file_path ä¸ºæ–°è§£å‹å‡ºçš„ç›®å½•, ä»¥ä¾¿ä¸‹æ¬¡å¾ªç¯
      file_path="$sub_dir"
    elif file "$file_path" | grep -qi 'Zip archive data'; then
      compressed_file="$file_path"
      log_safe "ğŸ—œï¸ zip å‹ç¼©åŒ…, è§£å‹ä¸­..."
      sub_dir=$(mktemp -d "$target_dir/decompress.XXXXXX")
      unzip -o "$compressed_file" -d "$sub_dir" >/dev/null 2>&1
      rm -f "$compressed_file"
      file_path="$sub_dir"
    else
      # å¦‚æœç›®å½•ä¸­è¿˜æœ‰å…¶ä»–å‹ç¼©æ–‡ä»¶, é€’å½’å¤„ç†
      next_archive=$(find "$target_dir" -type f \( -name "*.zip" -o -name "*.tar.gz" -o -name "*.gz" \) | head -n 1)
      if [ -n "$next_archive" ]; then
        file_path="$next_archive"
        continue
      fi
      # æ²¡æœ‰ä»»ä½•å¯è§£å‹çš„æ–‡ä»¶äº†
      break
    fi
  done

  # å†æ¬¡åœ¨æ•´ä¸ªè§£å‹ç›®å½•ä¸­å¯»æ‰¾ç›®æ ‡æ–‡ä»¶
  found_path=$(find "$target_dir" -type f -iname "$BIN_NAME" | head -n 1)
  [ -n "$found_path" ] && echo "$found_path" && return 0

  return 1
}

# --- ä¸»é€»è¾‘ ---

log_safe "âœ¨ === [fetch-core] === âœ¨"
log_safe "ğŸš€ å¼€å§‹è·å–æ ¸å¿ƒ: $BIN_REPO"
log_safe "ğŸ’» ç›®æ ‡æ¶æ„: ${ARCHITECTURE:-'è‡ªåŠ¨åŒ¹é…'}"
log_safe "ğŸ“› ç›®æ ‡äºŒè¿›åˆ¶: $BIN_NAME"

# GitHub Token
if [ -f "$PERSIST_DIR/github_token" ]; then
  GHTOKEN=$(tr -d '\r\n' <"$PERSIST_DIR/github_token" 2>/dev/null)
  [ -n "$GHTOKEN" ] && AUTH_HDR="Authorization: token $GHTOKEN"
fi

# --- è§£æ BIN_REPO ---
# æ ¼å¼: workflow@owner/repo/branch æˆ– owner/repo
IS_WORKFLOW=false
if echo "$BIN_REPO" | grep -q '@'; then
  IS_WORKFLOW=true
  WORKFLOW_BRANCH=$(echo "$BIN_REPO" | cut -d'/' -f3)
  REPO_SLUG=$(echo "$BIN_REPO" | cut -d'@' -f2 | cut -d'/' -f1,2)
  API_URL_BASE="https://api.github.com/repos/${REPO_SLUG}"
  log_safe "ğŸŒ€ å·¥ä½œæµæ¨¡å¼: ä» ${REPO_SLUG} çš„ ${WORKFLOW_BRANCH} åˆ†æ”¯è·å–"
else
  REPO_SLUG="$BIN_REPO"
  API_URL_BASE="https://api.github.com/repos/${REPO_SLUG}"
  log_safe "ğŸ·ï¸ Release æ¨¡å¼: ä» ${REPO_SLUG} è·å–"
fi

# --- å¯»æ‰¾ä¸‹è½½é“¾æ¥ ---
ASSET_URL=""

if [ "$IS_WORKFLOW" = "true" ]; then
  # --- æ¨¡å¼ä¸€: ä» GitHub Actions Artifacts ä¸‹è½½ ---
  log_safe "ğŸ“¡ æŸ¥è¯¢æœ€æ–°çš„æˆåŠŸå·¥ä½œæµ..."
  WORKFLOWS_API="${API_URL_BASE}/actions/runs?branch=${WORKFLOW_BRANCH}&status=success&per_page=1"
  retry_curl "$WORKFLOWS_API" "$TMPDIR/workflows.json" || exit 1

  LATEST_RUN_ID=$(grep -o '"id": *[0-9]*' "$TMPDIR/workflows.json" | head -n 1 | grep -o '[0-9]*')
  [ -z "$LATEST_RUN_ID" ] && log_safe "âŒ æœªæ‰¾åˆ°ä»»ä½•æˆåŠŸçš„å·¥ä½œæµè¿è¡Œ" && exit 1
  log_safe "âœ… æ‰¾åˆ°æœ€æ–°æˆåŠŸçš„å·¥ä½œæµè¿è¡Œ ID: $LATEST_RUN_ID"

  ARTIFACTS_API="${API_URL_BASE}/actions/runs/${LATEST_RUN_ID}/artifacts"
  retry_curl "$ARTIFACTS_API" "$TMPDIR/artifacts.json" || exit 1

  log_safe "ğŸ”— è§£ææ„å»ºäº§ç‰©ä¸‹è½½é“¾æ¥..."
  # ä¼˜å…ˆåŒ¹é…æ¶æ„, å…¶æ¬¡åŒ¹é…é€šç”¨åç§°
  # å°†å•è¡Œ JSON æ‹†åˆ†ä¸ºå¤šè¡Œ, æ›´æ˜“äº grep å¤„ç†
  ARTIFACT_LIST=$(sed 's/},{/}\n{/g' "$TMPDIR/artifacts.json")

  # ä¼˜å…ˆåŒ¹é…æ¶æ„
  ASSET_URL=$(echo "$ARTIFACT_LIST" | grep -i '"name":"[^"]*'"$ARCHITECTURE"'["]*"' | sed 's/.*\"archive_download_url\":\"\([^\"]*\)\".*/\1/' | head -n 1)
  # å…¶æ¬¡åŒ¹é…é€šç”¨åç§°
  [ -z "$ASSET_URL" ] && ASSET_URL=$(echo "$ARTIFACT_LIST" | grep -i '"name":"[^"]*'"$BIN_NAME"'["]*"' | sed 's/.*\"archive_download_url\":\"\([^\"]*\)\".*/\1/' | head -n 1)

else
  # --- æ¨¡å¼äºŒ: ä» GitHub Releases ä¸‹è½½ ---
  log_safe "ğŸ“¡ æŸ¥è¯¢æœ€æ–°çš„ Release..."
  JSON_FILE="$TMPDIR/release.json"
  RELEASE_API="$API_URL_BASE/releases/latest"
  retry_curl "$RELEASE_API" "$JSON_FILE" || {
    log_safe "âš ï¸ è·å–æœ€æ–° Release å¤±è´¥, å°è¯•è·å–æ‰€æœ‰ Release åˆ—è¡¨..."
    JSON_FILE="$TMPDIR/releases.json"
    RELEASE_API="$API_URL_BASE/releases"
    retry_curl "$RELEASE_API" "$JSON_FILE" || exit 1
  }

  log_safe "ğŸ”— è§£æ Release èµ„æºä¸‹è½½é“¾æ¥..."
  ALL_URLS=$(grep -o '"browser_download_url":"[^"]*"' "$JSON_FILE" | awk -F '"' '{print $4}')
  ASSET_URL=$(echo "$ALL_URLS" | awk -v arch="$ARCHITECTURE" 'tolower($0) ~ tolower(arch) { print; exit }')
  [ -z "$ASSET_URL" ] && ASSET_URL=$(echo "$ALL_URLS" | awk -v name="$BIN_NAME" 'tolower($0) ~ tolower(name) { print; exit }')
  [ -z "$ASSET_URL" ] && ASSET_URL=$(echo "$ALL_URLS" | head -n 1) # æœ€åæ‰‹æ®µ, æ‹¿ç¬¬ä¸€ä¸ª
fi

[ -z "$ASSET_URL" ] && log_safe "âŒ æœªæ‰¾åˆ°ä»»ä½•åˆé€‚çš„ä¸‹è½½é“¾æ¥" && exit 1
log_safe "âœ… ç¡®å®šä¸‹è½½é“¾æ¥: $ASSET_URL"

# --- ä¸‹è½½ä¸è§£å‹ ---
FNAME="$TMPDIR/asset.download"
log_safe "ğŸ“¥ æ­£åœ¨ä¸‹è½½..."
retry_curl "$ASSET_URL" "$FNAME" || exit 1

log_safe "ğŸ“¦ ä¸‹è½½å®Œæˆ, å¼€å§‹æ™ºèƒ½è§£å‹..."
BPATH=$(decompress_recursively "$FNAME" "$TMPDIR")

[ -z "$BPATH" ] && log_safe "âŒ åœ¨ä¸‹è½½çš„èµ„æºä¸­æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: $BIN_NAME" && exit 1

# --- éªŒè¯ä¸å®‰è£… ---
log_safe "âœ… æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: $BPATH"
chmod 755 "$BPATH"
VER=$("$BPATH" version 2>/dev/null | awk '/version/ {sub(/.*version /, ""); sub(/^v/, ""); print $1}')
[ -n "$VER" ] && log_safe "â„¹ï¸ æ ¸å¿ƒç‰ˆæœ¬: $VER"

mv "$BPATH" "$OUTPUT_PATH"
chmod 755 "$OUTPUT_PATH"
log_safe "ğŸ‰ æˆåŠŸå°† $BIN_NAME å®‰è£…åˆ° $OUTPUT_PATH"

exit 0
